{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPrest57OrkF51EMvhF3ZYW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brkived/Sure_PPO/blob/main/PPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CoOwreo1-VB",
        "outputId": "0cace8e7-1410-4903-91f7-ca74281f7ae3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Youtube-Code-Repository'...\n",
            "remote: Enumerating objects: 1044, done.\u001b[K\n",
            "remote: Counting objects: 100% (220/220), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 1044 (delta 202), reused 187 (delta 183), pack-reused 824\u001b[K\n",
            "Receiving objects: 100% (1044/1044), 43.11 MiB | 12.33 MiB/s, done.\n",
            "Resolving deltas: 100% (587/587), done.\n",
            "/content/Youtube-Code-Repository/ReinforcementLearning/PolicyGradient/PPO/torch\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/philtabor/Youtube-Code-Repository.git\n",
        "%cd Youtube-Code-Repository/ReinforcementLearning/PolicyGradient/PPO/torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iGzisoxqFtvX"
      },
      "outputs": [],
      "source": [
        "# This is the environment code for the Sure plus Summer 2024.\n",
        "# Sara, Maitha, Batool, and Rand: Kindly note that I will update it reguraly, so\n",
        "# whenever you run the simulation go and copy this code to your code\n",
        "# Author: Dr. Abdulmalik Alwarafy, PhD. Summer 2024\n",
        "#       *************************************************************\n",
        "from gym import spaces\n",
        "from gym.utils import seeding\n",
        "import numpy as np # malik: a library for scientific computing\n",
        "import cvxpy as cvx # maik library for optimization in python\n",
        "import pandas as pd\n",
        "#import colorama\n",
        "#from colorama import Fore, Back, Style\n",
        "import random\n",
        "import copy\n",
        "\n",
        "import os, time, sys # malik handels models savings\n",
        "import numpy as np\n",
        "import math\n",
        "import torch as T\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import mlab\n",
        "import gym\n",
        "from numpy import linalg as LA\n",
        "\n",
        "# import Box2D  # maik I added this, I first \"pip install box2d or box2d-py\", then restarted kernel, related to LunarLanderContinuous-v2 env\n",
        "# import winsound # for peep sound\n",
        "# np.set_printoptions(precision=4)\n",
        "# np.random.seed(0) # malik I added this\n",
        "#!pip install box2d-py # a repackaged version of pybox2d\n",
        "\n",
        "# malik at the beggining, make 2 directories \"tmp/ddpg\", manualy or typing followoing line in consol or in colab\n",
        "# !mkdir tmp && mkdir tmp/ddpg   # creating directory for the models\n",
        "\n",
        "#%% 1) Environment\n",
        "class WirelessEnvironment:\n",
        "    '''\n",
        "The code should include 3 main compoenents:\n",
        "1- Environment class: with intialization fucntion (of networks varialbles & state), step function (take action return s' & r)\n",
        "2- Agent class: with memory function, DNN function, learn function: agent is single ground center GC\n",
        "3- Main function: with main loopes and training\n",
        "\n",
        "    '''\n",
        "\n",
        "    def __init__(self, seed, No_TX_UAVs, No_Jam_UAVs, No_Eav):\n",
        "        # ---------------------------------\n",
        "        # 1) parameters related to UAVs, from the table in the paper\n",
        "        # self.seed(seed=seed)\n",
        "        self.set_seed(seed=seed)\n",
        "        # np.random.seed(seed)\n",
        "        self.No_TX_UAVs = No_TX_UAVs                   # number of transmit UAVs\n",
        "        self.No_Jam_UAVs = No_Jam_UAVs                   # number of jamming UAVs\n",
        "        self.No_Eav = No_Eav                   # number of ground eavesdroppers. I will consider them one\n",
        "        self.Tot_No_UAVs = self.No_TX_UAVs + self.No_Jam_UAVs\n",
        "\n",
        "        self.coverage_area = 200 # diameter in meter, not necessarly circular area. This x_min & y_min\n",
        "        self.height_min, self.height_max = 20, 80 # heights of transmittng and jamming UAVs\n",
        "\n",
        "        self.frequency= 2.4e9; # frequency of all RATs (in Hz)\n",
        "\n",
        "        self.Wl_max = 20e6; # max (or available) BW of all UAVs (in Hz)\n",
        "        # PSD or noise powers -174 dBm/Hz = 10^((-174-30)/10)\n",
        "        T = 290 ; B = 15000 ; k_Boltzmann = 1.38e-23; self.PSD = k_Boltzmann*T*B*1e0 #* np.ones(shape=self.L)# (watts)\n",
        "        self.exp_dist_mean = 2.46 # in my conference it was 2.46 dB mean of small-scale fading channel modeled as exponential RV (in dB)\n",
        "        self.shad_fact = 1.8 # shadowing factor X of the PL (in dB)\n",
        "        self.S_i = self.S_j = 0.5 # The harvesting efficiency of UAVs\n",
        "        self.UAVs_speed = 10 #  np.random.randint(5, 20, self.Tot_No_UAVs)* (1000/3600) # speeds of UAVs (in m/s)\n",
        "        self.Eav_speed = 2 # I assume eavesdropper speed is 2 m/s\n",
        "        self.R_min_sec = 1e6 # bps  np.zeros(shape=(self.No_TX_UAVs, self.No_Eav), dtype=np.float32) # secrecy rate\n",
        "        self.SNR_min_eav = 1.6 # dB\n",
        "        self.B_max = 500# Jouls Maximum battery capacity of UAVs.\n",
        "        UAVs_max_power_dBm = 30 ; self.P_max = 10**((UAVs_max_power_dBm - 30)/10) # max power in of trasmitting & Jamming UAVs in dBm\n",
        "        self.d_min = 10 # m this is the in between distances between UAVs. I will consider transmitting UAVs\n",
        "        self.pf = 1 # this is the flight power\n",
        "        self.L = 10 # minimum distance between UAVs\n",
        "\n",
        "        self.x_TX_UAVs = np.random.uniform(1, self.coverage_area, self.No_TX_UAVs) # Initial 3D locations of transmitting UAVs are random.\n",
        "        self.y_TX_UAVs = np.random.uniform(1, self.coverage_area, self.No_TX_UAVs)\n",
        "        self.z_TX_UAVs = np.random.uniform(self.height_min, self.height_max, self.No_TX_UAVs) # the z coordinates represents the height\n",
        "        self.TX_UAVs_locations = np.stack([self.x_TX_UAVs, self.y_TX_UAVs, self.z_TX_UAVs], axis=1) # appending locations of trasmitting UAVs (to be optimized)\n",
        "\n",
        "        self.x_Jam_UAVs = np.random.uniform(1, self.coverage_area, self.No_Jam_UAVs) # Initial 3D locations of jamming UAVs are random, we should optimize them\n",
        "        self.y_Jam_UAVs = np.random.uniform(1, self.coverage_area, self.No_Jam_UAVs)\n",
        "        self.z_Jam_UAVs = np.random.uniform(self.height_min, self.height_max, self.No_Jam_UAVs)\n",
        "        self.Jam_UAVs_locations = np.stack([self.x_Jam_UAVs, self.y_Jam_UAVs, self.z_Jam_UAVs], axis=1) # appending locations of jamming UAVs (to be optimized)\n",
        "\n",
        "        self.x_Eav = np.random.uniform(1, self.coverage_area, self.No_Eav) # Initial 3D locations of ground eavesdroppers are random (I assume one eav)\n",
        "        self.y_Eav = np.random.uniform(1, self.coverage_area, self.No_Eav)\n",
        "        self.z_Eav = 1.5# np.random.uniform(0, 1.5, self.No_Eav)\n",
        "        self.Eav_locations = np.stack([self.x_Eav, self.y_Eav, [1.5] ], axis=1) # appending locations of eavesdropper\n",
        "\n",
        "        self.Gr_cen_location = np.array([self.coverage_area/2, self.coverage_area/2, 1.5]) # locatoin of ground center is middle with height 1.5 m\n",
        "        self.Char_sta_location = np.array([self.coverage_area/2, self.coverage_area/2, 3]) # locatoin of charging station is middle with height 3 m\n",
        "\n",
        "        self.pi = np.ones(shape=(self.No_TX_UAVs, 1)) # vector of power allocatoin of transmitting UAVs. To be optimized\n",
        "        self.pj = np.ones(shape=(self.No_Jam_UAVs, 1)) # vector of power allocatoin of jamming UAVs. To be optimized\n",
        "\n",
        "        self.w = self.Wl_max/(self.No_TX_UAVs+self.No_Jam_UAVs) # bandwidth of all UAVs\n",
        "        self.e_i_h = np.ones(shape=(self.No_TX_UAVs, 1)) # vector of harvested/transfered energy from charging station to transmitting UAVs. To be optimized\n",
        "        self.e_j_h = np.ones(shape=(self.No_Jam_UAVs, 1)) # vector of harvested/transfered energy from charging station to transmitting UAVs. To be optimized\n",
        "        self.b_i = np.ones(shape=(self.No_TX_UAVs, 1)) # energies intialization. Energy level\n",
        "        self.b_j = np.ones(shape=(self.No_Jam_UAVs, 1)) # energies intialization\n",
        "        self.E_i_h = np.ones(shape=(self.No_TX_UAVs, 1))\n",
        "        self.E_j_h = np.ones(shape=(self.No_Jam_UAVs, 1))\n",
        "        self.e_i_t = np.ones(shape=(self.No_TX_UAVs, 1)) # vector of transmitting energy of TX UAV\n",
        "        self.e_j_t = np.ones(shape=(self.No_Jam_UAVs, 1)) # vector of transmitting energy of Jam UAV\n",
        "        self.e_i_f = np.ones(shape=(self.No_TX_UAVs, 1)) # vector of flight energy of TX UAV\n",
        "        self.e_j_f = np.ones(shape=(self.No_Jam_UAVs, 1)) # vector of flight energy of Jam UAV\n",
        "\n",
        "        self.RiG = np.zeros(shape=(self.No_TX_UAVs, 1)) # legitimate rate\n",
        "        self.Rie = np.zeros(shape=(self.No_TX_UAVs, self.No_Eav)) # illegitimate rate\n",
        "\n",
        "        self.l = np.random.uniform(100, 200)*1e6 # transmitted packet size is from 100 to 200 Mbps. I assume Ii and Ij the same\n",
        "\n",
        "        self.update_links()\n",
        "\n",
        "\n",
        "    def calculate_distances(self):\n",
        "      self.diG = LA.norm(self.TX_UAVs_locations - self.Gr_cen_location, axis=1) # distances between TX UAVs and ground center\n",
        "      self.die = LA.norm(self.TX_UAVs_locations - self.Eav_locations, axis=1) # distances between TX UAVs and eavesdropper\n",
        "      self.dje = LA.norm(self.Jam_UAVs_locations - self.Eav_locations, axis=1) # distances between jamming UAVs and eavesdropper\n",
        "      self.dCi = LA.norm(self.Char_sta_location - self.TX_UAVs_locations, axis=1) # distances between charging station & TX UAVs\n",
        "      self.dCj = LA.norm(self.Char_sta_location - self.Jam_UAVs_locations, axis=1) # distances between charging station & jamming UAVs\n",
        "      a = np.zeros(shape=(self.No_TX_UAVs, self.No_Jam_UAVs)) # the dij matrix is NtxNj\n",
        "      for j in range(self.No_Jam_UAVs):\n",
        "        a[:, j] = LA.norm(self.TX_UAVs_locations - self.Jam_UAVs_locations[j], axis=1) # distances between TX UAVs & Jamming UAVs\n",
        "      self.dij = a # LA.norm(self.TX_UAVs_locations - self.Jam_UAVs_locations, axis=1) # the dij matrix is NtxNj\n",
        "\n",
        "\n",
        "    def calculate_gains_1(self):\n",
        "      # generate channel gains: hiG (Ntx1), hie (NtxNe), hje (NjxNe)\n",
        "      self.hiG = np.zeros(shape=(self.No_TX_UAVs, 1)) # channel between transmitting UAVs & ground station\n",
        "      self.hie = np.zeros(shape=(self.No_TX_UAVs, self.No_Eav)) # channel between transmitting UAVs & eavesdropper\n",
        "      self.hje = np.zeros(shape=(self.No_Jam_UAVs, self.No_Eav)) # channel between jamming UAVs & eavesdropper\n",
        "      self.hCi = np.zeros(shape=(self.No_TX_UAVs, 1)) # channel between charging station & TX UAVs\n",
        "      self.hCj = np.zeros(shape=(self.No_Jam_UAVs, 1)) # channel between charging station & jamming UAVs\n",
        "      p_LoS_ij = PL_NLOS_ij = np.zeros(shape=(self.No_TX_UAVs, self.No_Jam_UAVs))\n",
        "      self.hij = np.zeros(shape=(self.No_TX_UAVs, self.No_Jam_UAVs)) # channel between jamming UAVs &\n",
        "\n",
        "      ALoS, ANLoS, reference_dist, beta, alpha = 10, 20, 1, 9.6, 0.28 #\n",
        "      p_LoS_iG = 1/(1 + beta * np.exp(-alpha*(180/np.pi * np.arctan(self.TX_UAVs_locations[:, 2]/self.diG) - beta))) # probability of LoS. vector Nt\n",
        "      p_LoS_ie = 1/(1 + beta * np.exp(-alpha*(180/np.pi * np.arctan(self.TX_UAVs_locations[:, 2]/self.die) - beta))) # probability of LoS. vector Nt\n",
        "      p_LoS_je = 1/(1 + beta * np.exp(-alpha*(180/np.pi * np.arctan(self.Jam_UAVs_locations[:, 2]/self.dje) - beta))) # probability of LoS. vector Nj\n",
        "      p_LoS_Ci = 1/(1 + beta * np.exp(-alpha*(180/np.pi * np.arctan(self.TX_UAVs_locations[:, 2]/self.dCi) - beta))) # probability of LoS. vector Nj\n",
        "      p_LoS_Cj = 1/(1 + beta * np.exp(-alpha*(180/np.pi * np.arctan(self.Jam_UAVs_locations[:, 2]/self.dCj) - beta))) # probability of LoS. vector Nj\n",
        "      a = np.zeros(shape=(self.No_TX_UAVs, self.No_Jam_UAVs)) # for the the p_LoS_ij matrix NtxNj\n",
        "      for j in range(self.No_Jam_UAVs): # from jammer to TX UAVs\n",
        "        a[:, j] = 1/(1 + beta * np.exp(-alpha*(180/np.pi * np.arctan(self.TX_UAVs_locations[:, 2]/self.dij[:, j]) - beta)))\n",
        "      p_LoS_ij = a# 1/(1 + beta * np.exp(-alpha*(180/np.pi * np.arctan(self.TX_UAVs_locations[:, 2]/self.dij) - beta))) # probability of LoS. vector Nj\n",
        "\n",
        "      PL_LOS_iG = ALoS + 20 * np.log10(4 * np.pi * self.frequency * self.diG/3e8) # LoS path-loss in dB. Vector size Nt\n",
        "      PL_NLOS_iG = ANLoS + 20 * np.log10(4 * np.pi * self.frequency * self.diG/3e8) # NLoS path-loss in dB.\n",
        "      PL_LOS_ie = ALoS + 20 * np.log10(4 * np.pi * self.frequency * self.die/3e8) # LoS path-loss in dB. Vector size Nt\n",
        "      PL_NLOS_ie = ANLoS + 20 * np.log10(4 * np.pi * self.frequency * self.die/3e8) # NLoS path-loss in dB.\n",
        "      PL_LOS_je = ALoS + 20 * np.log10(4 * np.pi * self.frequency * self.dje/3e8) # LoS path-loss in dB. Vector size Nt\n",
        "      PL_NLOS_je = ANLoS + 20 * np.log10(4 * np.pi * self.frequency * self.dje/3e8) # NLoS path-loss in dB.\n",
        "      PL_LOS_Ci = ALoS + 20 * np.log10(4 * np.pi * self.frequency * self.dCi/3e8) # LoS path-loss in dB. Vector size Nt\n",
        "      PL_NLOS_Ci = ANLoS + 20 * np.log10(4 * np.pi * self.frequency * self.dCi/3e8) # NLoS path-loss in dB.\n",
        "      PL_LOS_Cj = ALoS + 20 * np.log10(4 * np.pi * self.frequency * self.dCj/3e8) # LoS path-loss in dB. Vector size Nt\n",
        "      PL_NLOS_Cj = ANLoS + 20 * np.log10(4 * np.pi * self.frequency * self.dCj/3e8) # NLoS path-loss in dB.\n",
        "      PL_LOS_ij = ALoS + 20 * np.log10(4 * np.pi * self.frequency * self.dij/3e8) # LoS path-loss in dB. Vector size NtxNr\n",
        "      PL_NLOS_ij = ANLoS + 20 * np.log10(4 * np.pi * self.frequency * self.dij/3e8) # NLoS path-loss in dB.\n",
        "\n",
        "      hhiG = np.random.exponential(10**(self.exp_dist_mean/10), size = self.No_TX_UAVs) # small scale fading for the RF channel\n",
        "      hhie = np.random.exponential(10**(self.exp_dist_mean/10), size = self.No_TX_UAVs)\n",
        "      hhje = np.random.exponential(10**(self.exp_dist_mean/10), size = self.No_Jam_UAVs)\n",
        "      hhCi = np.random.exponential(10**(self.exp_dist_mean/10), size = self.No_TX_UAVs)\n",
        "      hhCj = np.random.exponential(10**(self.exp_dist_mean/10), size = self.No_Jam_UAVs)\n",
        "      hhij = np.random.exponential(10**(self.exp_dist_mean/10), size = (self.No_TX_UAVs, self.No_Jam_UAVs))\n",
        "\n",
        "      self.PL_iG = p_LoS_iG * PL_LOS_iG + (1 - p_LoS_iG) * PL_NLOS_iG # + np.random.normal(0, 10**(self.shad_fact/10), size=self.No_TX_UAVs) #path loss TX UAV & Ground BS # PL + shadowing in dB (0 mean & std =shad_fac)\n",
        "      self.PL_ie = p_LoS_ie * PL_LOS_ie + (1 - p_LoS_ie) * PL_NLOS_ie # + np.random.normal(0, 10**(self.shad_fact/10), size=self.No_TX_UAVs)\n",
        "      self.PL_je = p_LoS_je * PL_LOS_je + (1 - p_LoS_je) * PL_NLOS_je # + np.random.normal(0, 10**(self.shad_fact/10), size=self.No_Jam_UAVs)\n",
        "      self.PL_Ci = p_LoS_Ci * PL_LOS_Ci + (1 - p_LoS_Ci) * PL_NLOS_Ci # + np.random.normal(0, 10**(self.shad_fact/10), size=self.No_TX_UAVs)\n",
        "      self.PL_Cj = p_LoS_Cj * PL_LOS_Cj + (1 - p_LoS_Cj) * PL_NLOS_Cj # + np.random.normal(0, 10**(self.shad_fact/10), size=self.No_Jam_UAVs)\n",
        "      self.PL_ij = p_LoS_ij * PL_LOS_ij + (1 - p_LoS_ij) * PL_NLOS_ij # + np.random.normal(0, 10**(self.shad_fact/10), size=(self.No_TX_UAVs, self.No_Jam_UAVs))\n",
        "\n",
        "      self.hiG = np.power(10, -(self.PL_iG/10)) * (np.abs(hhiG))**2 # maybe with or without the second term (* (np.abs(hhiG))**2)\n",
        "      self.hie = np.power(10, -(self.PL_ie/10)) * (np.abs(hhie))**2\n",
        "      self.hje = np.power(10, -(self.PL_je/10)) * (np.abs(hhje))**2\n",
        "      self.hCi = np.power(10, -(self.PL_Ci/10)) * (np.abs(hhCi))**2\n",
        "      self.hCj = np.power(10, -(self.PL_Cj/10)) * (np.abs(hhCj))**2\n",
        "      self.hij = np.power(10, -(self.PL_ij/10)) * (np.abs(hhij))**2\n",
        "      print(\"*********hhiG\", hhiG, \"*******self.hiG\", self.hiG)\n",
        "\n",
        "\n",
        "    def calculate_gains(self):\n",
        "      # generate channel gains: hiG (Ntx1), hie (NtxNe), hje (NjxNe)\n",
        "      #self.hiG = np.zeros(shape=(self.No_TX_UAVs, 1)) # channel between transmitting UAVs & ground station\n",
        "      #self.hie = np.zeros(shape=(self.No_TX_UAVs, self.No_Eav)) # channel between transmitting UAVs & eavesdropper\n",
        "      #self.hje = np.zeros(shape=(self.No_Jam_UAVs, self.No_Eav)) # channel between jamming UAVs & eavesdropper\n",
        "      #self.hCi = np.zeros(shape=(self.No_TX_UAVs, 1)) # channel between charging station & TX UAVs\n",
        "      #self.hCj = np.zeros(shape=(self.No_Jam_UAVs, 1)) # channel between charging station & jamming UAVs\n",
        "      #self.hij = np.zeros(shape=(self.No_TX_UAVs, self.No_Jam_UAVs)) # channel between jamming UAVs &\n",
        "\n",
        "      PLE_LOS, reference_dist = 2, 1 # PL(dlu) = PLFS(d0)(dB) + 10nlog10( dlu/d0) +X(dB)\n",
        "      FSPL = 20 * np.log10(4 * np.pi * self.frequency * reference_dist/3e8) # path-loss at a close-in reference distance d0\n",
        "      hhiG = np.random.exponential(10**(self.exp_dist_mean/10), size = self.No_TX_UAVs) # small scale fading for the RF channel\n",
        "      hhie = np.random.exponential(10**(self.exp_dist_mean/10), size = self.No_TX_UAVs)\n",
        "      hhje = np.random.exponential(10**(self.exp_dist_mean/10), size = self.No_Jam_UAVs)\n",
        "      hhCi = np.random.exponential(10**(self.exp_dist_mean/10), size = self.No_TX_UAVs)\n",
        "      hhCj = np.random.exponential(10**(self.exp_dist_mean/10), size = self.No_Jam_UAVs)\n",
        "      hhij = np.random.exponential(10**(self.exp_dist_mean/10), size = (self.No_TX_UAVs, self.No_Jam_UAVs)) # NtxNj\n",
        "      self.PL_iG = FSPL + 10*(PLE_LOS) * np.log10(self.diG/reference_dist) + np.random.normal(0, 10**(self.shad_fact/10), size=self.No_TX_UAVs) # PL + shadowing in dB (0 mean & std =shad_fac)\n",
        "      self.PL_ie = FSPL + 10*(PLE_LOS) * np.log10(self.die/reference_dist) + np.random.normal(0, 10**(self.shad_fact/10), size=self.No_TX_UAVs)\n",
        "      self.PL_je = FSPL + 10*(PLE_LOS) * np.log10(self.dje/reference_dist) + np.random.normal(0, 10**(self.shad_fact/10), size=self.No_Jam_UAVs)\n",
        "      self.PL_Ci = FSPL + 10*(PLE_LOS) * np.log10(self.dCi/reference_dist) + np.random.normal(0, 10**(self.shad_fact/10), size=self.No_TX_UAVs)\n",
        "      self.PL_Cj = FSPL + 10*(PLE_LOS) * np.log10(self.dCj/reference_dist) + np.random.normal(0, 10**(self.shad_fact/10), size=self.No_Jam_UAVs)\n",
        "      self.PL_ij = FSPL + 10*(PLE_LOS) * np.log10(self.dij/reference_dist) + np.random.normal(0, 10**(self.shad_fact/10), size=(self.No_TX_UAVs, self.No_Jam_UAVs))\n",
        "      # print(\"mlaik check this ====>\", hhiG,\"------\", self.PL_iG)\n",
        "\n",
        "      self.hiG = np.power(10, -(self.PL_iG/10)) * (np.abs(hhiG))**2\n",
        "      self.hie = np.power(10, -(self.PL_ie/10)) * (np.abs(hhie))**2\n",
        "      self.hje = np.power(10, -(self.PL_je/10)) * (np.abs(hhje))**2\n",
        "      self.hCi = np.power(10, -(self.PL_Ci/10)) * (np.abs(hhCi))**2\n",
        "      self.hCj = np.power(10, -(self.PL_Cj/10)) * (np.abs(hhCj))**2\n",
        "      self.hij = np.power(10, -(self.PL_ij/10)) * (np.abs(hhij))**2\n",
        "\n",
        "\n",
        "    def calculate_SNRs(self):\n",
        "      # calculate SNRs which need power, gains, BWs, and PSDs ==> gamma = h*p/(w*segma**2)\n",
        "      self.gamma_iG = (self.hiG * np.reshape(self.pi, -1))/(self.w * self.PSD) # vector of length TX UAVs\n",
        "      print(\"mlaik self.gamma_iG ====>\", self.gamma_iG)\n",
        "      self.gamma_ie = (self.hie * self.pi)/(self.w * self.PSD**2 + np.sum(self.hje * self.pj)) # from the conference paper\n",
        "      a = np.zeros(shape=(self.No_TX_UAVs, self.No_Jam_UAVs)) # the dij matrix is NtxNj\n",
        "      for j in range(self.No_Jam_UAVs): # from jammer to TX UAVs\n",
        "        a[:, j] = (self.hij[:, j] * self.pj[j])/(self.w * self.PSD)\n",
        "      self.gamma_ij = a\n",
        "\n",
        "      self.I_tot = np.sum(np.log10(self.gamma_ij)) # the interference is assmed to be SNR\n",
        "\n",
        "\n",
        "    def calculate_rates(self):\n",
        "      # calculate the rates\n",
        "      self.RiG = self.w * np.log2(1 + self.gamma_iG) # vector of length TX UAVs\n",
        "      print(\"mlaik self.RiG ====>\", self.RiG)\n",
        "      self.Rie = self.w * np.log2(1 + self.gamma_ie) # vector of length TX UAVs\n",
        "      self.Ri_sec = self.RiG - self.Rie # secrecy rate. vector of length TX UAVs\n",
        "      print(\"mlaik self.RiG ====>\", self.RiG, \"---self.Rie=\", self.Rie)\n",
        "      self.Rsec_tot = np.sum(self.Ri_sec) # total secrecy rate of all TX UAVs\n",
        "\n",
        "\n",
        "    def calculate_energies(self):\n",
        "      #self.b_i = np.minimum( (self.b_i + self.E_i_h - self.e_i_t - self.e_i_f) , self.B_max)\n",
        "      #self.b_j = np.minimum( (self.b_j + self.E_j_h - self.e_j_t - self.e_j_f) , self.B_max)\n",
        "      self.e_i_t = (np.reshape(self.pi, -1) * self.l)/self.RiG # transmiting energy from TX UAVs. Vector of length Nt\n",
        "      self.e_j_t = (np.reshape(self.pj, -1) * self.l)/np.sum(self.Rie) # transmiting energy from jamming UAVs. vector of length Nj. I have added the summation here\n",
        "\n",
        "      self.e_i_f = (self.pf/self.UAVs_speed) * LA.norm(self.TX_UAVs_locations - self.TX_UAVs_locations, axis=1) # flight energy of TX UAVs. Vector of length Nt. To be updated\n",
        "      self.e_j_f = (self.pf/self.UAVs_speed) * LA.norm(self.Jam_UAVs_locations - self.Jam_UAVs_locations, axis=1) # flight energy of jamming UAVs. Vector of length Nj\n",
        "\n",
        "      self.e_i_h = np.ones(shape=(self.No_TX_UAVs, 1)) # harvested energy by TX UAVs. This is one of our actions. Vector Nt\n",
        "      self.e_j_h = np.ones(shape=(self.No_Jam_UAVs, 1)) # harvested energy by Jam UAVs. This is one of our actions. Vector Nj\n",
        "\n",
        "      self.E_i_h = np.reshape(self.hCi, -1) * self.S_i * np.reshape(self.e_i_h, -1) # Energy consumed by ith UAV harvesting technique. Vector Nt. gHr Si e^Hr_i ,\n",
        "      self.E_j_h = self.hCj * self.S_j * np.reshape(self.e_j_h, -1) # Energy consumed by ith UAV harvesting technique. Vector Nt. gHr Si e^Hr_i ,\n",
        "\n",
        "      self.b_i = np.minimum( (np.reshape(self.b_i, -1) + self.E_i_h - self.e_i_t - self.e_i_f) , self.B_max) # vector Nt\n",
        "      self.b_j = np.minimum( (np.reshape(self.b_j, -1) + self.E_j_h - self.e_j_t - self.e_j_f) , self.B_max) # vector Nj\n",
        "\n",
        "      self.E_tot = np.sum((self.e_i_t + self.e_i_f + self.e_i_h)/self.pi) + np.sum((self.e_j_t + self.e_j_f + self.e_j_h)/self.pj) # total energy effeciency\n",
        "\n",
        "\n",
        "    def calculate_utility_function(self):\n",
        "      # thi function calculates the objective function of our OPB U = w1 Rsec_tot + w2 E_tot + w3 I_tot\n",
        "      self.w1, self.w2, self.w3 = 1,.1,.6# 8e-7, 1e-0, 1e-0\n",
        "      #self.U = self.w1 * self.Rsec_tot + self.w2 * self.E_tot - self.w3 * self.I_tot\n",
        "      #print(\"secrecy rate term utility=\", self.w1 * self.Rsec_tot, \"--- energy effeciency term utility=\", self.w2 * self.E_tot,\n",
        "      #      \"--- interference term utility=\", self.w3 * self.I_tot, \"-----total utility\", self.U)\n",
        "      a1= np.sum(self.Ri_sec/np.max(self.Ri_sec));\n",
        "      a2=(self.e_i_t + self.e_i_f + self.e_i_h)/self.pi ; a2 = np.sum(a2/np.max(a2))\n",
        "      a3 =(self.e_j_t + self.e_j_f + self.e_j_h)/self.pj ; a3 = np.sum(a3/np.max(a3))\n",
        "      a4=np.log10(self.gamma_ij) ; a4=np.sum(a4/np.max(a4))\n",
        "      self.U = self.w1 * a1 + self.w2 * (a2+a3) - self.w3 * a4 # another version of utility\n",
        "      print(\"secrecy rate term utility=\", self.w1 * a1, \"--- energy effeciency term utility=\", self.w2 * (a2+a3),\n",
        "            \"--- interference term utility=\", self.w3 * a4, \"-----total utility\", self.U)\n",
        "\n",
        "\n",
        "    def update_links(self):\n",
        "      # this function is called after each action at each time slot to regularly calculates the following parameters/functions:\n",
        "      self.calculate_distances() # update distances based on the new UAV locations action\n",
        "      # self.calculate_gains()     # update gains based on the new UAV distances\n",
        "      self.calculate_gains_1()\n",
        "      self.calculate_SNRs()      # update SNR based on the new UAV distances + power allocation action\n",
        "      self.calculate_rates()     # update rates based on the new SNRs\n",
        "      self.calculate_energies()  # update energies based on the new harvesting energy action\n",
        "      self.calculate_utility_function()  # update total system utility\n",
        "\n",
        "\n",
        "    def move_Eav_1(self): # malik I can use this for mobility model of eavesdropper, inspired by Faris\n",
        "        # move eavesdropper randomly then I have to recalculate gains to all UAVs for each new EAV location\n",
        "        theta = np.random.uniform(low=-np.pi, high=np.pi)\n",
        "        dx = self.Eav_speed * np.cos(theta);\n",
        "        dy = self.Eav_speed * np.sin(theta);\n",
        "        self.Eav_locations[0] += dx\n",
        "        self.Eav_locations[1] += dy\n",
        "        # self.Eav_locations = np.stack([self.x_Eav, self.y_Eav, 1.5], axis=0) # get the new location of eavesdropper\n",
        "\n",
        "\n",
        "    def move_Eav(self): # I may consider this\n",
        "        \"\"\"\n",
        "        malik this is the Random Waypoint Mobility Model for an eavesdropper in a 3D space.\n",
        "        time_step (float): The time step for each movement.\n",
        "        pause_time (float): The time to pause at each waypoint.\n",
        "        \"\"\"\n",
        "        pause_time = 0  # Start without pausing\n",
        "        time_step = 1.0  # seconds\n",
        "\n",
        "        if pause_time > 0: # If paused, return the old coordinates. I think I will delte this\n",
        "            pause_time -= time_step\n",
        "            return self.Eav_locations, pause_time\n",
        "\n",
        "        # Select a new random waypoint\n",
        "        new_x = np.random.uniform(0, self.converage_area) # xmin = 0, xmax= self.converage_area\n",
        "        new_y = np.random.uniform(0, self.converage_area)\n",
        "        # new_z = 1.5# np.random.uniform(zmin, zmax) # height of eavesdropper is 1.5m\n",
        "\n",
        "        # Calculate the distance to the new waypoint\n",
        "        distance = np.sqrt((new_x - self.Eav_locations[0])**2 + (new_y - self.Eav_locations[1])**2)\n",
        "        travel_time = distance / self.Eav_speed # Calculate the time required to reach the new waypoint\n",
        "\n",
        "        # Update coordinates based on speed and time_step\n",
        "        if travel_time > time_step:\n",
        "            ratio = time_step / travel_time\n",
        "            new_x = self.Eav_locations[0] + (new_x - self.Eav_locations[0]) * ratio\n",
        "            new_y = self.Eav_locations[1] + (new_y - self.Eav_locations[1]) * ratio\n",
        "            # new_z = 1.5 # self.Eav_locations[2] + (new_z - self.Eav_locations[2]) * ratio\n",
        "            pause_time = 0  # Not pausing yet\n",
        "        else:\n",
        "            pause_time = pause_time  # Pause at the new waypoint\n",
        "\n",
        "        self.Eav_locations = np.stack([new_x, new_y, 1.5], axis=0)\n",
        "\n",
        "\n",
        "    def set_seed(self, seed): # malik from there\n",
        "        random.seed(seed) # Set the seed for random\n",
        "        np.random.seed(seed) # Set the seed for numpy\n",
        "        # Set the seed for torch\n",
        "        T.manual_seed(seed)\n",
        "        T.cuda.manual_seed(seed)\n",
        "        T.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
        "        # Ensure reproducibility in torch.backends\n",
        "        T.backends.cudnn.deterministic = True\n",
        "        T.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "\n",
        "    def reset_state(self):\n",
        "      # S = [R_sec(t-1) (Ntx1), b(t-1) (Nt+Njx1), E^H(t-1)(Nt+Njx1), R^sec_min(1x1)] (3Nt + 2Nj +1)\n",
        "      # np.random.seed(0) ; random.seed(0)\n",
        "      a1 = (self.R_min_sec/self.No_TX_UAVs) * np.ones(self.No_TX_UAVs)\n",
        "      a2 = np.ones(self.Tot_No_UAVs)\n",
        "      a3 = np.ones(self.Tot_No_UAVs)\n",
        "      a4 = np.array([self.R_min_sec])\n",
        "      #self.state = np.concatenate( (a1, a2, a3, np.reshape(a4, (1,1)) ) , axis = 0) # malik this one was working\n",
        "      self.state = np.concatenate( (a1, a2, a3, a4 ) , axis = 0)\n",
        "      #self.state_spac = [ (self.R_min_sec/self.No_TX_UAVs) * np.ones(self.No_TX_UAVs), # R_sec(t-1) (Ntx1)\n",
        "      #                    np.ones(self.Tot_No_UAVs), # b(t-1) (Nt+Njx1)\n",
        "      #                    np.ones(self.Tot_No_UAVs), # E(t-1)(N_t+N_jx1)\n",
        "      #                    np.reshape(np.array(self.R_min_sec), (1,1)) # R^sec_min(1x1)\n",
        "      #                            ]\n",
        "      #self.state = np.array(self.state_spac, dtype=object)\n",
        "      #self.state = np.concatenate( (self.state_spac), axis = 0)\n",
        "      # print(\",,,,,,,,,,,,\", np.shape(self.state))\n",
        "      return self.state\n",
        "\n",
        "\n",
        "    def check_constraints(self):  # malik I want to check the constraints\n",
        "        self.Ri_sec_constraints = np.sum(self.Ri_sec > self.R_min_sec) == self.No_TX_UAVs # if this equals to 1, then C1 is met\n",
        "        self.SNR_eav_constraints = np.sum(self.gamma_ie < self.SNR_min_eav) == self.No_TX_UAVs # if this equals to 1, then C2 is met\n",
        "\n",
        "        a = np.zeros(shape=(self.No_TX_UAVs, self.No_TX_UAVs)) # distances between UAVs\n",
        "        for i in range(self.No_TX_UAVs):\n",
        "          for j in range(self.No_TX_UAVs):\n",
        "            if i == j:\n",
        "              a[i, j] = 0\n",
        "            else:\n",
        "              a[i, j] = LA.norm(self.TX_UAVs_locations[i] - self.TX_UAVs_locations[j], axis=0)\n",
        "        self.in_between_distances = np.copy(a)\n",
        "        self.d_min_constraints = np.sum(self.in_between_distances >= self.d_min) ==  (self.No_TX_UAVs*(self.No_TX_UAVs-1)) # if this equals to 1, then C7 is met\n",
        "\n",
        "        self.max_travel_distance = LA.norm(self.TX_UAVs_locations - selold_TX_UAV_locatoins, axis=1) # current and past locatoins of TX UAVs. Ntx1\n",
        "        # print(\"new locations:\\n\", self.TX_UAVs_locations, \"old locations:\\n\", old_TX_UAV_locatoins)\n",
        "        self.L_constraint = np.sum(self.max_travel_distance <= self.L) == self.No_TX_UAVs # if this equals to 1, then C6 is met\n",
        "\n",
        "        print(\"Rate ConstraintC1:\", self.Ri_sec_constraints, \"SNR ConstraintC2:\", self.SNR_eav_constraints, \"dmin ConstraintC7:\",\n",
        "              self.d_min_constraints, \"Lmin ConstraintC6:\", self.L_constraint)\n",
        "        self.costraints = self.Ri_sec_constraints & self.SNR_eav_constraints & self.d_min_constraints & self.L_constraint  # result is 1 or 0\n",
        "\n",
        "\n",
        "    def DDPG_step(self, action): # our second main function\n",
        "        self.reward_inst = 0.0 # instantionous reward\n",
        "        self.done, self.abort = False, False\n",
        "\n",
        "        state1 = self.state  # track old states if u might need them\n",
        "        # S(t) = [R_sec(t-1) (Ntx1), b(t-1) (Nt+Njx1), E^H(t-1)(Nt+Njx1), R^sec_min(1x1)] (3Nt + 2Nj +1) ;\n",
        "        R_sec = state1[0:self.No_TX_UAVs] # get current states\n",
        "        b = state1[self.No_TX_UAVs : (2*self.No_TX_UAVs + self.No_Jam_UAVs)]\n",
        "        E = state1[(2*self.No_TX_UAVs + self.No_Jam_UAVs) : (3*self.No_TX_UAVs + 2*self.No_Jam_UAVs)]\n",
        "        R_sec_min = state1[(3*self.No_TX_UAVs + 2*self.No_Jam_UAVs) : (3*self.No_TX_UAVs + 2*self.No_Jam_UAVs+1)]# self.R_min_sec\n",
        "\n",
        "        # A(t) = [Q (Nt+Nj x3), p (Nt+Nj x1), e^h (Nt+Nj x1)] 5(Nt+Nj)\n",
        "        # the action should be a vector ==> [(Ntx3) + (Njx3) + (Nt) + (Nj) + (Nt) + (Nj)]\n",
        "        # action = np.concatenate([a_TX_x, a_TX_y, a_TX_z, a_Jam_x, a_Jam_y, a_Jam_z, a_TX_p, a_Jam_p, a_TX_eh, a_Jam_eh], axis=0) # this is from main function\n",
        "        TX_UAVs_locations = action[0 : 3*self.No_TX_UAVs] # get TX UAVs locations. Vector of length Ntx3\n",
        "        Jam_UAVs_locations = action[3*self.No_TX_UAVs : 3*(self.No_TX_UAVs+self.No_Jam_UAVs)] # get Jam UAVs locations. Vector of length Njx3\n",
        "        TX_UAVs_powers = action[3*(self.No_TX_UAVs+self.No_Jam_UAVs) : (4*self.No_TX_UAVs + 3*self.No_Jam_UAVs)] # get TX UAVs powers. Vector of length Nt\n",
        "        Jam_UAVs_powers = action[(4*self.No_TX_UAVs + 3*self.No_Jam_UAVs) : 4*(self.No_TX_UAVs + self.No_Jam_UAVs)] # get Jam UAVs powers. Vector of length Nj\n",
        "        TX_UAVs_harv_ener = action[4*(self.No_TX_UAVs + self.No_Jam_UAVs) : (5*self.No_TX_UAVs + 4*self.No_Jam_UAVs)] # get TX UAVs harvesting energies. Vector of length Nt\n",
        "        Jam_UAVs_harv_ener = action[(5*self.No_TX_UAVs + 4*self.No_Jam_UAVs) : 5*(self.No_TX_UAVs + self.No_Jam_UAVs)] # get Jam UAVs harvesting energies. Vector of length Nj\n",
        "\n",
        "        self.TX_UAVs_locations = np.reshape(TX_UAVs_locations, (self.No_TX_UAVs, 3), order='F') # convert vector into Ntx3 matrix\n",
        "        # print(\"malik new locations===========>\\n\", self.TX_UAVs_locations)\n",
        "        self.Jam_UAVs_locations = np.reshape(Jam_UAVs_locations, (self.No_Jam_UAVs, 3), order='F') # convert vector into Njx3 matrix\n",
        "        self.pi = np.array(TX_UAVs_powers) # transmissoin power for trasmitting UAVs Nt\n",
        "        self.pj = np.array(Jam_UAVs_powers) # transmissoin power for jamming UAVs Nj\n",
        "        self.e_i_h = np.array(TX_UAVs_harv_ener) # harvested energy by TX UAVs. Vector Nt\n",
        "        self.e_j_h = np.array(Jam_UAVs_harv_ener) # harvested energy by jam UAVs. Vector Nj\n",
        "\n",
        "        self.update_links() # update all results/calculations based on the actions\n",
        "        self.check_constraints() # check constraints\n",
        "        # self.reward_inst += self.w1 * self.Rsec_tot + self.w2 * self.E_tot - self.w3 * self.I_tot # Initial: should be equal to the utiltiy function self.U\n",
        "        self.reward_inst += self.U # reward is equal to the utiltiy function self.U\n",
        "\n",
        "        if self.costraints:\n",
        "          self.done = True\n",
        "          self.abort = False\n",
        "          # self.reward_inst += 0.0\n",
        "        else:\n",
        "          self.done = False\n",
        "          self.abort = True\n",
        "          # self.reward_inst += -1e2\n",
        "\n",
        "          # now returen next state, reward, done, abort\n",
        "          # S = [R_sec(t-1) (Ntx1), b(t-1) (Nt+Njx1), E^H(t-1)(Nt+Njx1), R^sec_min(1x1)] (3Nt + 2Nj +1)\n",
        "          # print(\"mlaik see this ====>\", np.shape(self.b_i), \"mlaik see this ====>\", np.shape(self.b_j))\n",
        "          b = np.concatenate((self.b_i, self.b_j), axis = 0) # b(t-1) (Nt+Njx1)\n",
        "          E = np.concatenate((self.E_i_h, self.E_j_h), axis = 0) # E(t-1) (Nt+Njx1)\n",
        "\n",
        "          self.state = np.concatenate( (self.Ri_sec, b, E, np.reshape(self.R_min_sec, -1)) , axis = 0)\n",
        "\n",
        "        return self.state, self.reward_inst, self.done#, self.abort_RATs\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##ppo code\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch as T\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions.categorical import Categorical\n",
        "\n",
        "class PPOMemory:\n",
        "    def __init__(self, batch_size):\n",
        "        self.states = []\n",
        "        self.probs = []\n",
        "        self.vals = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def generate_batches(self):\n",
        "        n_states = len(self.states)\n",
        "        batch_start = np.arange(0, n_states, self.batch_size)\n",
        "        indices = np.arange(n_states, dtype=np.int64)\n",
        "        np.random.shuffle(indices)\n",
        "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
        "\n",
        "        return np.array(self.states),\\\n",
        "                np.array(self.actions),\\\n",
        "                np.array(self.probs),\\\n",
        "                np.array(self.vals),\\\n",
        "                np.array(self.rewards),\\\n",
        "                np.array(self.dones),\\\n",
        "                batches\n",
        "\n",
        "    def store_memory(self, state, action, probs, vals, reward, done):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.probs.append(probs)\n",
        "        self.vals.append(vals)\n",
        "        self.rewards.append(reward)\n",
        "        self.dones.append(done)\n",
        "\n",
        "    def clear_memory(self):\n",
        "        self.states = []\n",
        "        self.probs = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "        self.vals = []\n",
        "\n",
        "class ActorNetwork(nn.Module):\n",
        "    def __init__(self, n_actions, input_dims, alpha,\n",
        "            fc1_dims=256, fc2_dims=256, chkpt_dir='tmp/ppo'):\n",
        "        super(ActorNetwork, self).__init__()\n",
        "\n",
        "        self.checkpoint_file = os.path.join(chkpt_dir, 'actor_torch_ppo')\n",
        "        self.actor = nn.Sequential(\n",
        "                nn.Linear(*input_dims, fc1_dims),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(fc1_dims, fc2_dims),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(fc2_dims, n_actions),\n",
        "                nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
        "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, state):\n",
        "        dist = self.actor(state)\n",
        "        dist = Categorical(dist)\n",
        "\n",
        "        return dist\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        T.save(self.state_dict(), self.checkpoint_file)\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        self.load_state_dict(T.load(self.checkpoint_file))\n",
        "\n",
        "class CriticNetwork(nn.Module):\n",
        "    def __init__(self, input_dims, alpha, fc1_dims=256, fc2_dims=256,\n",
        "            chkpt_dir='tmp/ppo'):\n",
        "        super(CriticNetwork, self).__init__()\n",
        "\n",
        "        self.checkpoint_file = os.path.join(chkpt_dir, 'critic_torch_ppo')\n",
        "        self.critic = nn.Sequential(\n",
        "                nn.Linear(*input_dims, fc1_dims),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(fc1_dims, fc2_dims),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(fc2_dims, 1)\n",
        "        )\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
        "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, state):\n",
        "        value = self.critic(state)\n",
        "\n",
        "        return value\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        T.save(self.state_dict(), self.checkpoint_file)\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        self.load_state_dict(T.load(self.checkpoint_file))\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, n_actions, input_dims, gamma=0.99, alpha=0.0003, gae_lambda=0.95,\n",
        "            policy_clip=0.2, batch_size=64, n_epochs=10):\n",
        "        self.gamma = gamma\n",
        "        self.policy_clip = policy_clip\n",
        "        self.n_epochs = n_epochs\n",
        "        self.gae_lambda = gae_lambda\n",
        "\n",
        "        self.actor = ActorNetwork(n_actions, input_dims, alpha)\n",
        "        self.critic = CriticNetwork(input_dims, alpha)\n",
        "        self.memory = PPOMemory(batch_size)\n",
        "\n",
        "    def remember(self, state, action, probs, vals, reward, done):\n",
        "        self.memory.store_memory(state, action, probs, vals, reward, done)\n",
        "\n",
        "    def save_models(self):\n",
        "        print('... saving models ...')\n",
        "        self.actor.save_checkpoint()\n",
        "        self.critic.save_checkpoint()\n",
        "\n",
        "    def load_models(self):\n",
        "        print('... loading models ...')\n",
        "        self.actor.load_checkpoint()\n",
        "        self.critic.load_checkpoint()\n",
        "\n",
        "    def choose_action(self, observation):\n",
        "        state = T.tensor([observation], dtype=T.float).to(self.actor.device)\n",
        "\n",
        "        dist = self.actor(state)\n",
        "        value = self.critic(state)\n",
        "        action = dist.sample()\n",
        "\n",
        "        probs = T.squeeze(dist.log_prob(action)).item()\n",
        "        action = T.squeeze(action).item()\n",
        "        value = T.squeeze(value).item()\n",
        "\n",
        "        return action, probs, value\n",
        "\n",
        "    def learn(self):\n",
        "        for _ in range(self.n_epochs):\n",
        "            state_arr, action_arr, old_prob_arr, vals_arr,\\\n",
        "            reward_arr, dones_arr, batches = \\\n",
        "                    self.memory.generate_batches()\n",
        "\n",
        "            values = vals_arr\n",
        "            advantage = np.zeros(len(reward_arr), dtype=np.float32)\n",
        "\n",
        "            for t in range(len(reward_arr)-1):\n",
        "                discount = 1\n",
        "                a_t = 0\n",
        "                for k in range(t, len(reward_arr)-1):\n",
        "                    a_t += discount*(reward_arr[k] + self.gamma*values[k+1]*\\\n",
        "                            (1-int(dones_arr[k])) - values[k])\n",
        "                    discount *= self.gamma*self.gae_lambda\n",
        "                advantage[t] = a_t\n",
        "            advantage = T.tensor(advantage).to(self.actor.device)\n",
        "\n",
        "            values = T.tensor(values).to(self.actor.device)\n",
        "            for batch in batches:\n",
        "                states = T.tensor(state_arr[batch], dtype=T.float).to(self.actor.device)\n",
        "                old_probs = T.tensor(old_prob_arr[batch]).to(self.actor.device)\n",
        "                actions = T.tensor(action_arr[batch]).to(self.actor.device)\n",
        "\n",
        "                dist = self.actor(states)\n",
        "                critic_value = self.critic(states)\n",
        "\n",
        "                critic_value = T.squeeze(critic_value)\n",
        "\n",
        "                new_probs = dist.log_prob(actions)\n",
        "                prob_ratio = new_probs.exp() / old_probs.exp()\n",
        "                #prob_ratio = (new_probs - old_probs).exp()\n",
        "                weighted_probs = advantage[batch] * prob_ratio\n",
        "                weighted_clipped_probs = T.clamp(prob_ratio, 1-self.policy_clip,\n",
        "                        1+self.policy_clip)*advantage[batch]\n",
        "                actor_loss = -T.min(weighted_probs, weighted_clipped_probs).mean()\n",
        "\n",
        "                returns = advantage[batch] + values[batch]\n",
        "                critic_loss = (returns-critic_value)**2\n",
        "                critic_loss = critic_loss.mean()\n",
        "\n",
        "                total_loss = actor_loss + 0.5*critic_loss\n",
        "                self.actor.optimizer.zero_grad()\n",
        "                self.critic.optimizer.zero_grad()\n",
        "                total_loss.backward()\n",
        "                self.actor.optimizer.step()\n",
        "                self.critic.optimizer.step()\n",
        "\n",
        "        self.memory.clear_memory()\n"
      ],
      "metadata": {
        "id": "TvlqU4mH3Zys"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##main code\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "from ppo_torch import Agent\n",
        "from utils import plot_learning_curve\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    seed = 100\n",
        "    np.random.seed(seed)\n",
        "    env = WirelessEnvironment(seed=seed, No_TX_UAVs=10, No_Jam_UAVs=2, No_Eav=1)\n",
        "    N = 20\n",
        "    max_step = 200\n",
        "    normalize_state = 0\n",
        "    evalute = False\n",
        "    window = -100\n",
        "    state_size = 3 * env.No_TX_UAVs + 2 * env.No_Jam_UAVs + 1\n",
        "    action_size = 5 * (env.No_TX_UAVs + env.No_Jam_UAVs)\n",
        "    batch_size = 5\n",
        "    n_epochs = 4\n",
        "    alpha = 0.0003\n",
        "    agent = Agent(n_actions=action_size, batch_size=batch_size,\n",
        "                  alpha=alpha, n_epochs=n_epochs,\n",
        "                  input_dims=[state_size])\n",
        "    n_games = 20\n",
        "\n",
        "\n",
        "    figure_file = 'plots/cartpole.png'\n",
        "\n",
        "    best_score = -1000000\n",
        "    score_history = []\n",
        "\n",
        "    learn_iters = 0\n",
        "    avg_score = 0\n",
        "    n_steps = 0\n",
        "\n",
        "    old_TX_UAV_locations = np.copy(env.TX_UAVs_locations)\n",
        "\n",
        "    for i in range(n_games):\n",
        "        observation = env.reset_state()\n",
        "        done = False\n",
        "        score = 0\n",
        "        while not done:\n",
        "            action, prob, val = agent.choose_action(observation)\n",
        "            observation_, reward, done, info = env.DDPG_step(action)\n",
        "            n_steps += 1\n",
        "            score += reward\n",
        "            agent.remember(observation, action, prob, val, reward, done)\n",
        "            if n_steps % N == 0:\n",
        "                agent.learn()\n",
        "                learn_iters += 1\n",
        "            observation = observation_\n",
        "        score_history.append(score)\n",
        "        avg_score = np.mean(score_history[-100:])\n",
        "\n",
        "        if avg_score > best_score:\n",
        "            best_score = avg_score\n",
        "            agent.save_models()\n",
        "\n",
        "        print('episode', i, 'score %.1f' % score, 'avg score %.1f' % avg_score,\n",
        "                'time_steps', n_steps, 'learning_steps', learn_iters)\n",
        "    x = [i+1 for i in range(len(score_history))]\n",
        "    plot_learning_curve(x, score_history, figure_file)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "E08zxtwI3coO",
        "outputId": "784f0765-6fdb-45ab-cc0c-99038ac21eec"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*********hhiG [1.51556416 2.39039658 1.75272483 1.53626055 0.03638639 0.41539638\n",
            " 1.38626201 2.58276839 0.50852298 0.59330356] *******self.hiG [2.35665697e-09 5.16919430e-09 3.48074324e-09 8.55919241e-10\n",
            " 1.95013893e-13 3.62489919e-11 7.73095262e-10 3.00226264e-09\n",
            " 1.76323774e-10 8.21743019e-10]\n",
            "mlaik self.gamma_iG ====> [2.35547923e+01 5.16661099e+01 3.47900374e+01 8.55491495e+00\n",
            " 1.94916434e-03 3.62308765e-01 7.72708908e+00 3.00076226e+01\n",
            " 1.76235657e+00 8.21332353e+00]\n",
            "mlaik self.RiG ====> [7.69655452e+06 9.53133833e+06 8.60247691e+06 5.42707172e+06\n",
            " 4.68218786e+03 7.43422874e+05 5.20916754e+06 8.25758502e+06\n",
            " 2.44316592e+06 5.33953613e+06]\n",
            "mlaik self.RiG ====> [7.69655452e+06 9.53133833e+06 8.60247691e+06 5.42707172e+06\n",
            " 4.68218786e+03 7.43422874e+05 5.20916754e+06 8.25758502e+06\n",
            " 2.44316592e+06 5.33953613e+06] ---self.Rie= [[5.62050287e+05 4.80222246e+06 3.68538251e+06 2.02325987e+06\n",
            "  9.17157810e+04 1.37049662e+04 2.70670841e+04 4.91226800e+05\n",
            "  1.21629417e+04 1.41427243e+03]\n",
            " [5.62050287e+05 4.80222246e+06 3.68538251e+06 2.02325987e+06\n",
            "  9.17157810e+04 1.37049662e+04 2.70670841e+04 4.91226800e+05\n",
            "  1.21629417e+04 1.41427243e+03]\n",
            " [5.62050287e+05 4.80222246e+06 3.68538251e+06 2.02325987e+06\n",
            "  9.17157810e+04 1.37049662e+04 2.70670841e+04 4.91226800e+05\n",
            "  1.21629417e+04 1.41427243e+03]\n",
            " [5.62050287e+05 4.80222246e+06 3.68538251e+06 2.02325987e+06\n",
            "  9.17157810e+04 1.37049662e+04 2.70670841e+04 4.91226800e+05\n",
            "  1.21629417e+04 1.41427243e+03]\n",
            " [5.62050287e+05 4.80222246e+06 3.68538251e+06 2.02325987e+06\n",
            "  9.17157810e+04 1.37049662e+04 2.70670841e+04 4.91226800e+05\n",
            "  1.21629417e+04 1.41427243e+03]\n",
            " [5.62050287e+05 4.80222246e+06 3.68538251e+06 2.02325987e+06\n",
            "  9.17157810e+04 1.37049662e+04 2.70670841e+04 4.91226800e+05\n",
            "  1.21629417e+04 1.41427243e+03]\n",
            " [5.62050287e+05 4.80222246e+06 3.68538251e+06 2.02325987e+06\n",
            "  9.17157810e+04 1.37049662e+04 2.70670841e+04 4.91226800e+05\n",
            "  1.21629417e+04 1.41427243e+03]\n",
            " [5.62050287e+05 4.80222246e+06 3.68538251e+06 2.02325987e+06\n",
            "  9.17157810e+04 1.37049662e+04 2.70670841e+04 4.91226800e+05\n",
            "  1.21629417e+04 1.41427243e+03]\n",
            " [5.62050287e+05 4.80222246e+06 3.68538251e+06 2.02325987e+06\n",
            "  9.17157810e+04 1.37049662e+04 2.70670841e+04 4.91226800e+05\n",
            "  1.21629417e+04 1.41427243e+03]\n",
            " [5.62050287e+05 4.80222246e+06 3.68538251e+06 2.02325987e+06\n",
            "  9.17157810e+04 1.37049662e+04 2.70670841e+04 4.91226800e+05\n",
            "  1.21629417e+04 1.41427243e+03]]\n",
            "secrecy rate term utility= 53.493275736748025 --- energy effeciency term utility= 1.4132865228607099 --- interference term utility= 4.569262955760044 -----total utility 50.33729930384869\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'int' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-72f7e361c5b5>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mobservation_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDDPG_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mn_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-647e0568199b>\u001b[0m in \u001b[0;36mDDPG_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;31m# the action should be a vector ==> [(Ntx3) + (Njx3) + (Nt) + (Nj) + (Nt) + (Nj)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0;31m# action = np.concatenate([a_TX_x, a_TX_y, a_TX_z, a_Jam_x, a_Jam_y, a_Jam_z, a_TX_p, a_Jam_p, a_TX_eh, a_Jam_eh], axis=0) # this is from main function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0mTX_UAVs_locations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNo_TX_UAVs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# get TX UAVs locations. Vector of length Ntx3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m         \u001b[0mJam_UAVs_locations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNo_TX_UAVs\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNo_TX_UAVs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNo_Jam_UAVs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# get Jam UAVs locations. Vector of length Njx3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mTX_UAVs_powers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNo_TX_UAVs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNo_Jam_UAVs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNo_TX_UAVs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNo_Jam_UAVs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# get TX UAVs powers. Vector of length Nt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##utils\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_learning_curve(x, scores, figure_file):\n",
        "    running_avg = np.zeros(len(scores))\n",
        "    for i in range(len(running_avg)):\n",
        "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
        "    plt.plot(x, running_avg)\n",
        "    plt.title('Running average of previous 100 scores')\n",
        "    plt.savefig(figure_file)"
      ],
      "metadata": {
        "id": "tFzrl7g93mjZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}